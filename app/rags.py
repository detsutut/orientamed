from typing import Any, Literal, Tuple

from boto3 import Session
from langchain_aws import BedrockEmbeddings, InMemoryVectorStore, ChatBedrockConverse
from langchain_core.messages import BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import logging
from languagemodel import LanguageModel
from retriever import Retriever
from langgraph.graph import StateGraph, END
from langgraph.types import Command
from langchain_core.messages.human import HumanMessage
from typing_extensions import List, TypedDict
import textwrap
import json
from concept_mapper import SNOMEDMapper
from kg_retriever import shortest_path_id, get_chunk, shortest_path_bewteen
from langchain_core.documents import Document

logger = logging.getLogger('app.'+__name__)


def messages_to_history_str(messages: list[BaseMessage]) -> str:
    """Convert messages to a history string."""
    string_messages = []
    for message in messages:
        role = message.type
        content = message.content
        string_message = f"{role}: {content}"

        additional_kwargs = message.additional_kwargs
        if additional_kwargs:
            string_message += f"\n{additional_kwargs}"
        string_messages.append(string_message)
    return "\n".join(string_messages)


class Prompts:
    def __init__(self, jsonfile: str):
        with open(jsonfile, 'r') as file:
            data = json.load(file)
            for k, v in data.items():
                self.__setattr__(k, self.__parse__(v))

    def __parse__(self, prompt_dicts: List[dict]):
        return ChatPromptTemplate([(d["role"], d["content"]) for d in prompt_dicts])


# Define state for application
from typing_extensions import Annotated
from operator import add
class State(TypedDict):
    #INPUTS
    query: str # the user query
    history: List[BaseMessage] # all the interactions between ai and user
    additional_context: str # additional info added by the user to be considered a valid source
    query_aug: bool # use or not query augmentation technique before passing the query to the retriever
    #INTERNAL
    translation: str # output of translator
    answer_generated: bool
    #OUTPUTS
    input_tokens_count: Annotated[int, add] # amount of input tokens processed by the whole chain of llm calls triggered in this round
    output_tokens_count: Annotated[int, add] # amount of output tokens processed by the whole chain of llm calls triggered in this round
    answer: str # textual answer generated by the system and returned to the user
    query_concepts: list[dict] # list of concepts extracted from input query
    answer_concepts: list[dict]  # list of concepts extracted from generated answer
    kg_context: dict # retrieved documents to use as context source
    context: dict # retrieved documents to use as context source

class Rag:
    NORETRIEVE_MSG = "Mi dispiace, non sono riuscito a trovare informazioni rilevanti nelle linee guida."
    NOTALLOWED_MSG = "Mi dispiace, non posso rispondere a questa domanda."

    def __init__(self, session: Session,
                 model: ChatBedrockConverse | str,
                 embedder: BedrockEmbeddings | str,
                 vector_store: InMemoryVectorStore | str | None = None,
                 **kwargs):
        self.prompts = Prompts(kwargs.get("promptfile", "./prompts.json"))
        self.session = session
        client = session.client("bedrock-runtime", region_name=kwargs.get("region"))
        self.llm = LanguageModel(model, client=client, model_low=kwargs.get("model_low", None),
                                 model_pro=kwargs.get("model_pro", None))
        self.retriever = Retriever(embedder, vector_store=vector_store, client=client)

        graph_builder = StateGraph(state_schema=State)
        graph_builder.set_entry_point("orchestrator")
        graph_builder.add_node("orchestrator", self.orchestrator)
        graph_builder.add_node("history_consolidator", self.history_consolidator)
        graph_builder.add_node("augmentator", self.augmentator)
        graph_builder.add_node("emb_retriever", self.emb_retriever)
        graph_builder.add_node("ans_generator", self.ans_generator)
        graph_builder.add_node("translator", self.translator)
        graph_builder.add_node("consistency_checker", self.consistency_checker)
        graph_builder.add_node("concept_extractor", self.concept_extractor)
        graph_builder.add_node("kg_retriever", self.kg_retriever)
        self.graph = graph_builder.compile()

    def generate_norag(self, input: str):
        logger.info(f"Generating...")
        messages = self.prompts.question_open.invoke({"question": input}).messages
        response = self.llm.generate(messages=messages)
        return {"answer": response.content,
                "input_tokens_count": response.usage_metadata["input_tokens"],
                "output_tokens_count": response.usage_metadata["output_tokens"]}

    def orchestrator(self, state: State) -> Command[Literal["augmentator", "emb_retriever", "translator", "history_consolidator"]]:
        logger.info(f"Dispatching request...")
        previous_user_interactions = [message for message in state["history"] if type(message) is HumanMessage]
        if len(previous_user_interactions) > 0:
            return Command(update={"answer_generated":False},
                           goto="history_consolidator")
        else:
            return Command(update={"answer_generated":False},
                           goto="augmentator" if state["query_aug"] else ["emb_retriever","translator"])

    def translator(self, state: State) -> Command[Literal["concept_extractor"]]:
        logger.info(f"Translating...")
        text_to_translate = state["query"] if not state["answer_generated"] else state["answer"]
        messages = self.prompts.translation.invoke({"text": text_to_translate}).messages
        response = self.llm.generate(messages=messages)
        logger.info(f"Translation: {response.content}")
        return Command(
            update={"translation": response.content,
                    "input_tokens_count": response.usage_metadata["input_tokens"],
                    "output_tokens_count": response.usage_metadata["output_tokens"]
                    },
            goto="concept_extractor",
        )

    def concept_extractor(self, state: State) -> Command[Literal["kg_retriever","consistency_checker"]]:
        logger.info(f"Extracting Concepts...")
        mapper = SNOMEDMapper("data/reuma_dict.csv")
        concepts = mapper.map_text_to_snomed(state["translation"], fuzzy_threshold=1, unique=True)
        if not state["answer_generated"]:
            return Command(
                update={"query_concepts": concepts},
                goto="kg_retriever",
            )
        else:
            return Command(
                update={"answer_concepts": concepts},
                goto="consistency_checker",
            )

    def emb_retriever(self, state: State) -> Command[Literal["ans_generator", END]]:
        logger.info(f"Retrieving Documents...")
        retrieved_docs, scores = self.retriever.retrieve_with_scores(state["query"], n=10, score_threshold=0.6)
        logger.info(f"Retrieved docs: {retrieved_docs}")
        additional_context = state.get("additional_context", None)
        if len(retrieved_docs) == 0 and (type(additional_context) is not str or additional_context == ""):
            return Command(
                update={"context": {"docs": retrieved_docs, "scores": scores},
                        "answer": self.NORETRIEVE_MSG},
                goto=END,
            )
        else:
            return Command(
                update={"context": {"docs": retrieved_docs, "scores": scores}},
                goto="ans_generator",
            )

    def consistency_checker(self, state: State) -> Command[Literal[END]]:
        logger.info(f"Checking answer consistency...")
        qc = state["query_concepts"]
        qc_names = [c["name"] for c in qc]
        qc_ids = [c["id"] for c in qc]
        ac = state["answer_concepts"]
        inconsistent_concepts = []
        answer_warning = ""
        for answer_concept in ac:
            if answer_concept["name"] not in qc_names:
                path_found = False
                for question_concept_id in qc_ids:
                    if shortest_path_bewteen(id1=answer_concept["id"], id2=question_concept_id, max_hops=5):
                        path_found = True
                        break
                if not path_found:
                    inconsistent_concepts.append(answer_concept["name"]+" ("+answer_concept["id"]+")")
        if inconsistent_concepts:
            ic_string = ', '.join(inconsistent_concepts)
            answer_warning = f"**INCONSISTENCY WARNING: concepts mentioned in the answer appear to be unlinked with query concepts. {ic_string}.\nThe generated answer should be carefully evaluated. Do not rely on the generated answer for medical decisions.**"
        return Command(
            update={"answer": state["answer"]+"\n"+answer_warning},
            goto=END,
        )

    def kg_retriever(self, state: State) -> Command[Literal["ans_generator"]]:
        logger.info(f"Retrieving Nodes...")
        paths = []
        for concept in state["query_concepts"]:
            paths.extend(shortest_path_id(id=concept["id"], max_hops=3))
        result = {}
        for path in paths:
            item_id = path["id"]
            if item_id not in result or path["nodeCount"] < result[item_id]["nodeCount"]:
                result[item_id] = path
        deduplicated_paths = list(result.values())
        deduplicated_sorted_paths = sorted(deduplicated_paths, key=lambda x: x["nodeCount"])
        retrieved_docs = []
        path_strings = []
        for path in deduplicated_sorted_paths:
            path_string = ""
            for element in path["path"]:
                if type(element) is dict:
                    path_string += "["+element.get("FSN","Chunk")+"]"
                elif type(element) is str:
                    path_string += "--["+element+"]--"
            path_strings.append(path_string)
            chunk = get_chunk(path["id"])
            retrieved_docs.append(Document(id=chunk["chunkId"], page_content=chunk["text"], metadata={"extra": True, "source": chunk["chunkId"]}))
        scores = [c["nodeCount"] for c in deduplicated_sorted_paths]
        return Command(
            update={"kg_context": {"docs": retrieved_docs, "scores": scores, "paths": path_strings}},
            goto="ans_generator",
        )


    def history_consolidator(self, state: State) -> Command[Literal["orchestrator"]]:
        logger.info(f"Consolidating history...")
        messages = self.prompts.history_consolidation.invoke({"question": state["query"],
                                                              "history": messages_to_history_str(
                                                                  state["history"])}).messages
        response = self.llm.generate(messages=messages)
        consolidated_query = response.content
        logger.info(f"Consolidated query: {textwrap.shorten(consolidated_query, width=30)}")
        return Command(
            update={"query": consolidated_query,
                    "history": [],
                    "input_tokens_count": response.usage_metadata["input_tokens"],
                    "output_tokens_count": response.usage_metadata["output_tokens"]},
            goto="orchestrator",
        )

    def augmentator(self, state: State) -> Command[Literal["emb_retriever","translator"]]:
        logger.info(f"Expanding Query...")
        messages = self.prompts.query_expansion.invoke({"question": state["query"]}).messages
        response = self.llm.generate(messages=messages)
        augmented_query = response.content
        logger.info(f"Expanded query: {textwrap.shorten(augmented_query, width=30)}")
        return Command(
            update={"query": augmented_query,
                    "input_tokens_count": response.usage_metadata["input_tokens"],
                    "output_tokens_count": response.usage_metadata["output_tokens"]
                    },
            goto=["emb_retriever","translator"],
        )

    def ans_generator(self, state: State) -> Command[Literal["translator"]]:
        logger.info(f"Generating...")
        doc_strings = []
        for i, doc in enumerate(state["context"]["docs"]):
            doc_strings.append(f"Source {i+1}:\n{doc.page_content}")
        additional_context = state.get("additional_context", None)
        if type(additional_context) is str and additional_context != "":
            logger.info(f"Appending additional context...")
            doc_strings.append(f"Source [0]:\n{additional_context}")
        docs_content = "\n".join(doc_strings)
        messages = self.prompts.question_with_context_inline_cit.invoke({"question": state["query"], "context": docs_content}).messages
        response = self.llm.generate(messages=messages, level="pro")
        return Command(update={"answer": response.content,
                               "answer_generated": True,
                               "input_tokens_count": response.usage_metadata["input_tokens"],
                               "output_tokens_count": response.usage_metadata["output_tokens"]},
                       goto="translator")

    def invoke(self, input: dict[str, Any]):
        return self.graph.invoke(input)

    def get_image(self):
        return self.graph.get_graph().draw_mermaid_png()
